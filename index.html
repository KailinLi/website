<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Kailin Li</title>

  <meta name="author" content="Kailin Li">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Kailin Li | ÊùéÊÅ∫Êûó</name>
                  </p>
                  <p>I'm a third-year Ph.D. student in the Department of <a style="color:#545454"
                      href="http://www.cs.sjtu.edu.cn/en/">Computer Science</a> at <a
                      href="http://en.sjtu.edu.cn">Shanghai Jiao Tong University</a>. Currently, I am a member of the
                    SJTU
                    <a href="https://www.mvig.org/">MVIG</a> lab under the supervision of Prof. <strong>Cewu
                      Lu</strong>.
                    Before that, I received my Bachelor's degree (<a style="color:#545454"
                      href="http://www.cs.hust.edu.cn/index.htm">Computer Science</a>) in 2019
                    from <a style="color:#545454" href="http://english.hust.edu.cn">Huazhong University of Science and
                      Technology</a>. My
                    research interests include <strong><em>Computer Vision</em></strong>,
                    <strong><em>3D Vision</em></strong>, and <strong><em>Robotics</em></strong>.
                  </p>


                  <p style="text-align:center">
                    <a href="mailto:kailinli@sjtu.edu.cn">Email</a> &nbsp/&nbsp
                    <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp -->
                    <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                    <a href="https://scholar.google.com/citations?user=zEDPB2MAAAAJ">Google Scholar</a>
                    &nbsp/&nbsp
                    <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp -->
                    <a href="https://github.com/KailinLi">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/kailin.jpg"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/kailin_circle.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <p>
                    <!-- I'm interested in computer vision, machine learning, optimization, and image processing. Much of my
                    research is about inferring the physical world (shape, motion, color, light, etc) from images.
                    Representative papers are <span class="highlight">highlighted</span>. -->
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="artiboost_stop()" onmouseover="artiboost_start()">
                <td style="padding:25px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='artiboost_image'>
                      <img src='images/artiboost_after.jpg' width="200">
                    </div>
                    <img src='images/artiboost_before.png' width="200">
                  </div>
                  <script type="text/javascript">
                    function artiboost_start() {
                      document.getElementById('artiboost_image').style.opacity = "1";
                    }

                    function artiboost_stop() {
                      document.getElementById('artiboost_image').style.opacity = "0";
                    }
                    artiboost_stop()
                  </script>
                </td>
                <td style="padding:25px;width:70%;vertical-align:middle">
                  <a href="#">
                    <papertitle>ArtiBoost: Boosting Articulated 3D Hand-Object Pose Estimation via Online Exploration
                      and Synthesis
                    </papertitle>
                  </a>
                  <br>
                  <a href="#">Lixin Yang</a>*,
                  <strong>Kailin Li</strong>*,
                  <a href="#">Xinyu Zhan</a>,
                  <a href="#">Jun Lv</a>, <br>
                  <a href="#">Wenqiang Xu</a>,
                  <a href="#">Jiefeng Li</a>,
                  <a href="#">Cewu Lu</a>
                  <br>
                  <em>CVPR</em>, 2022
                  <br>
                  <a href="#">project</a>
                  /
                  <a href="https://arxiv.org/abs/2109.05488">arxiv</a>
                  /
                  <a href="https://github.com/MVIG-SJTU/ArtiBoost">code</a>
                  <p></p>
                  <p>
                    We propose a lightweight online data enrichment method that boosts articulated hand-object pose
                    estimation
                    from the data perspective.
                    During training, ArtiBoost alternatively performs data exploration and synthesis.
                    Even with a simple baseline, our method can boost it to outperform the previous SOTA on several
                    hand-object benchmarks.</p>
                </td>
              </tr>

              <tr onmouseout="cpf_stop()" onmouseover="cpf_start()">
                <td style="padding:25px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='cpf_image'>
                      <img src='images/cpf_after.png' width="200">
                    </div>
                    <img src='images/cpf_before.jpg' width="200">
                  </div>
                  <script type="text/javascript">
                    function cpf_start() {
                      document.getElementById('cpf_image').style.opacity = "1";
                    }

                    function cpf_stop() {
                      document.getElementById('cpf_image').style.opacity = "0";
                    }
                    cpf_stop()
                  </script>
                </td>
                <td style="padding:25px;width:70%;vertical-align:middle">
                  <a href="https://lixiny.github.io/CPF">
                    <papertitle>CPF: Learning a Contact Potential Field to Model the Hand-Object Interaction
                    </papertitle>
                  </a>
                  <br>
                  <a href="#">Lixin Yang</a>,
                  <a href="#">Xinyu Zhan</a>,
                  <strong>Kailin Li</strong>, <br>
                  <a href="#">Wenqiang Xu</a>,
                  <a href="#">Jiefeng Li</a>,
                  <a href="#">Cewu Lu</a>
                  <br>
                  <em>ICCV</em>, 2021
                  <br>
                  <a href="https://lixiny.github.io/CPF">project</a>
                  /
                  <a
                    href="https://openaccess.thecvf.com/content/ICCV2021/html/Yang_CPF_Learning_a_Contact_Potential_Field_To_Model_the_Hand-Object_ICCV_2021_paper.html">paper</a>
                  /
                  <a
                    href="https://openaccess.thecvf.com/content/ICCV2021/supplemental/Yang_CPF_Learning_a_ICCV_2021_supplemental.pdf">supp</a>
                  /
                  <a href="https://arxiv.org/abs/2012.00924">arxiv</a>
                  /
                  <a href="https://github.com/lixiny/CPF">code</a>
                  /
                  <a href="https://zhuanlan.zhihu.com/p/406470702">Áü•‰πé</a>
                  <p></p>
                  <p>
                    We highlight contact in the hand-object interaction modeling task by proposing an
                    explicit representation named Contact Potential Field (CPF). In CPF, we treat each contacting
                    hand-object
                    vertex pair as a spring-mass system, Hence the whole system forms a potential field with minimal
                    elastic
                    energy
                    at the grasp position.</p>
                </td>
              </tr>

              <!-- <tr onmouseout="refnerf_stop()" onmouseover="refnerf_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='refnerf_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/refnerf.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/refnerf.jpg' width="160">
                  </div>
                  <script type="text/javascript">
                    function refnerf_start() {
                      document.getElementById('refnerf_image').style.opacity = "1";
                    }

                    function refnerf_stop() {
                      document.getElementById('refnerf_image').style.opacity = "0";
                    }
                    refnerf_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://dorverbin.github.io/refnerf/index.html">
                    <papertitle>Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields</papertitle>
                  </a>
                  <br>
                  <a href="https://scholar.harvard.edu/dorverbin/home">Dor Verbin</a>,
                  <a href="https://phogzone.com/">Peter Hedman</a>,
                  <a href="https://bmild.github.io/">Ben Mildenhall</a>, <br>
                  <a href="Todd Zickler">Todd Zickler</a>,
                  <strong>Jonathan T. Barron</strong>,
                  <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>
                  <br>
                  <em>CVPR</em>, 2022
                  <br>
                  <a href="https://dorverbin.github.io/refnerf/index.html">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2112.03907">arXiv</a>
                  /
                  <a href="https://youtu.be/qrdRH9irAlk">video</a>
                  <p></p>
                  <p>Explicitly modeling reflections in NeRF produces realistic shiny surfaces and accurate surface
                    normals, and lets you edit materials.</p>
                </td>
              </tr>

              <tr>

              <tr onmouseout="mipnerf_stop()" onmouseover="mipnerf_start()" bgcolor="#ffffd0">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='mipnerf_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/mipnerf_ipe_yellow.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/mipnerf_ipe_yellow.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function mipnerf_start() {
                      document.getElementById('mipnerf_image').style.opacity = "1";
                    }

                    function mipnerf_stop() {
                      document.getElementById('mipnerf_image').style.opacity = "0";
                    }
                    mipnerf_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="http://jonbarron.info/mipnerf">
                    <papertitle>Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields
                    </papertitle>
                  </a>
                  <br>
                  <strong>Jonathan T. Barron</strong>,
                  <a href="https://bmild.github.io/">Ben Mildenhall</a>,
                  <a href="http://matthewtancik.com/">Matthew Tancik</a>, <br>
                  <a href="https://phogzone.com/">Peter Hedman</a>,
                  <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>,
                  <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>
                  <br>
                  <em>ICCV</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Honorable
                      Mention)</strong></font>
                  <br>
                  <a href="http://jonbarron.info/mipnerf">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2103.13415">arXiv</a>
                  /
                  <a href="https://youtu.be/EpH175PY1A0">video</a>
                  /
                  <a href="https://github.com/google/mipnerf">code</a>
                  <p></p>
                  <p>NeRF is aliased, but we can anti-alias it by casting cones and prefiltering the positional encoding
                    function.</p>
                </td>
              </tr>

              <tr onmouseout="survey_stop()" onmouseover="survey_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='survey_image'>
                      <img src='images/survey_after.png' width="160">
                    </div>
                    <img src='images/survey_before.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function survey_start() {
                      document.getElementById('survey_image').style.opacity = "1";
                    }

                    function survey_stop() {
                      document.getElementById('survey_image').style.opacity = "0";
                    }
                    survey_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2111.05849">
                    <papertitle>Advances in Neural Rendering</papertitle>
                  </a>
                  <br>
                  <a href="https://people.mpi-inf.mpg.de/~atewari/">Ayush Tewari</a>,
                  <a href="https://justusthies.github.io/">Justus Thies</a>,
                  <a href="https://bmild.github.io/">Ben Mildenhall</a>,
                  <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
                  <a href="https://people.mpi-inf.mpg.de/~tretschk/">Edgar Tretschk</a>,
                  <a href="https://homes.cs.washington.edu/~yifan1/">Yifan Wang</a>,
                  <a href="https://christophlassner.de/">Christoph Lassner</a>,
                  <a href="https://vsitzmann.github.io/">Vincent Sitzmann</a>,
                  <a href="http://ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>,
                  <a href="https://stephenlombardi.github.io/">Stephen Lombardi</a>,
                  <a href="http://www.cs.cmu.edu/~tsimon/">Tomas Simon</a>,
                  <a href="https://www.mpi-inf.mpg.de/departments/visual-computing-and-artificial-intelligence">Christian
                    Theobalt</a>,
                  <a href="https://www.niessnerlab.org/">Matthias Niessner</a>,
                  <strong>Jonathan T. Barron</strong>,
                  <a href="https://stanford.edu/~gordonwz/">Gordon Wetzstein</a>,
                  <a href="https://zollhoefer.com/">Michael Zollhoefer</a>,
                  <a href="https://people.mpi-inf.mpg.de/~golyanik/">Vladislav Golyanik</a>
                  <br>
                  <em>Arxiv</em>, 2021
                  <br>
                  <p></p>
                  <p>
                    A survey of recent progress in neural rendering.
                  </p>
                </td>
              </tr>

              <tr onmouseout="jump_stop()" onmouseover="jump_start()" bgcolor="#ffffd0">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='jump_image'><img src='images/jump_anim.gif'></div>
                    <img src='images/jump_still.png'>
                  </div>
                  <script type="text/javascript">
                    function jump_start() {
                      document.getElementById('jump_image').style.opacity = "1";
                    }

                    function jump_stop() {
                      document.getElementById('jump_image').style.opacity = "0";
                    }
                    jump_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://drive.google.com/file/d/1RBnTrtzqmuO8uj3GQaR5vBJZjIC3Jxjn/view?usp=sharing">
                    <papertitle>Jump: Virtual Reality Video</papertitle>
                  </a>
                  <br>
                  <a href="http://mi.eng.cam.ac.uk/~ra312/">Robert Anderson</a>, <a
                    href="https://www.cs.unc.edu/~gallup/">David Gallup</a>, <strong>Jonathan T. Barron</strong>, <a
                    href="https://mediatech.aalto.fi/~janne/index.php">Janne Kontkanen</a>, <a
                    href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>, <a
                    href="http://carlos-hernandez.org/">Carlos Hern&aacutendez</a>, <a
                    href="https://homes.cs.washington.edu/~sagarwal/">Sameer Agarwal</a>, <a
                    href="https://homes.cs.washington.edu/~seitz/">Steven M Seitz</a>
                  <br>
                  <em>SIGGRAPH Asia</em>, 2016
                  <br>
                  <a
                    href="https://drive.google.com/file/d/11D4eCDXqqFTtZT0WS2COJE0hsAN3QEww/view?usp=sharing">supplement</a>
                  /
                  <a href="https://www.youtube.com/watch?v=O0qUYynupTI">video</a> /
                  <a href="data/Anderson2016.bib">bibtex</a> /
                  <a href="https://blog.google/products/google-vr/jump-using-omnidirectional-stereo-vr-video/">blog
                    post</a>
                  <p></p>
                  <p>Using computer vision and a ring of cameras, we can make video for virtual reality headsets that is
                    both stereo and 360&deg;.</p>
                  <p>This technology is used by <a href="https://vr.google.com/jump/">Jump</a>. </p>
                </td>
              </tr> -->

            </tbody>
          </table>

          <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>Misc</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
                <td width="75%" valign="center">
                  <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
                  <br>
                  <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Longuet-Higgins Award Committee Member,
                    CVPR 2021</a>
                  <br>
                  <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
                  <br>
                  <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/cs188.jpg" alt="cs188">
                </td>
                <td width="75%" valign="center">
                  <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor,
                    CS188 Spring 2011</a>
                  <br>
                  <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor,
                    CS188 Fall 2010</a>
                  <br>
                  <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd
                    Edition</a>
                </td>
              </tr>

              <tr>
                <td align="center" style="padding:20px;width:25%;vertical-align:middle">
                  <heading>Basically <br> Blog Posts</heading>
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                  <br>
                  <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain
                    Functions</a>
                  <br>
                  <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                </td>
              </tr>

            </tbody>
          </table> -->
        </td>
      </tr>
  </table>
</body>

</html>