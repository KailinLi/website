<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Kailin Li</title>

  <meta name="author" content="Kailin Li">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!-- <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>"> -->
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Kailin Li | ÊùéÊÅ∫Êûó</name>
                  </p>
                  <p>
                    I am currently a researcher at the <a href="https://internrobotics.shlab.org.cn">Embodied AI
                      Center</a>, <a href="https://www.shlab.org.cn"><strong>Shanghai
                        AI Laboratory</strong></a>. I received my Ph.D. in Computer
                    Science and Technology from <a href="http://en.sjtu.edu.cn">Shanghai Jiao Tong
                      University</a>, where I worked in the <a href="https://www.mvig.org/">MVIG Lab</a> under the
                    supervision of <a href="https://www.mvig.org/">Prof. Cewu Lu</a>.
                  </p>
                  <p>
                    I now collaborate closely with
                    <a href="https://wangjingbo1219.github.io">Jingbo Wang</a> and <a
                      href="https://oceanpang.github.io/">Jiangmiao Pang</a>,
                    and previously gained valuable experience at the General Vision Lab, <a
                      href="https://www.bigai.ai">BIGAI</a>.
                    My research focuses on <em><strong>loco-manipulation</strong>, unifying whole-body humanoid
                      locomotion with
                      dexterous-hand manipulation</em>.
                  </p>
                  <p>
                    <span style="color: rgb(255, 85, 0);">I am seeking highly
                      motivated interns to join our team at Shanghai AI Lab‚Äîinterested candidates are warmly encouraged
                      to reach out via email.</span>
                  </p>


                  <p style="text-align:center">
                    <span id="email-link"></span> &nbsp;/&nbsp;
                    <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp -->
                    <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                    <a href="https://scholar.google.com/citations?user=zEDPB2MAAAAJ">Google Scholar</a>
                    &nbsp/&nbsp
                    <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp -->
                    <a href="https://github.com/KailinLi">Github</a>
                  </p>
                  <script>
                    (function () {
                      const encoded = "a2FpbGlubGlAc2p0dS5lZHUuY24=";
                      const email = atob(encoded);
                      const holder = document.getElementById("email-link");
                      if (holder) {
                        const link = document.createElement("a");
                        link.href = `mailto:${email}`;
                        link.textContent = "Email";
                        holder.appendChild(link);
                      }
                    })();
                  </script>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/kailin-new.png"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/kailin-circle.png" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Publications</heading>
                  <p>
                    <!-- I'm interested in computer vision, machine learning, optimization, and image processing. Much of my
                    research is about inferring the physical world (shape, motion, color, light, etc) from images.
                    Representative papers are <span class="highlight">highlighted</span>. -->
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="gallant_stop()" onmouseover="gallant_start()">
                <td style="padding:10%;width:45%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='gallant_video'><video width="130%" muted autoplay loop>
                        <source src="images/gallant_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/gallant_teaser.pdf' width="125%">
                  </div>
                  <script type="text/javascript">
                    function gallant_start() {
                      document.getElementById('gallant_video').style.opacity = "1";
                    }

                    function gallant_stop() {
                      document.getElementById('gallant_video').style.opacity = "0";
                    }
                    gallant_stop()
                  </script>
                </td>
                <td style="padding:0px;width:55%;vertical-align:middle">
                  <a href="#">
                    <papertitle><span style="font-variant: small-caps;">Gallant</span>: Voxel Grid-based Humanoid
                      Locomotion and Local-navigation across 3D Constrained Terrains
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://www.qingweiben.com/">Qingwei Ben</a>*,
                  <a href="https://scholar.google.com/citations?user=OLLsgKIAAAAJ&hl=en">Botian Xu</a>*,
                  <strong>Kailin Li</strong>*,
                  <a href="https://trap-1.github.io/">Feiyu Jia</a>,
                  <a href="https://zwt006.github.io/">Wentao Zhang</a>,
                  <a href="https://scholar.google.com/citations?user=YmnK82wAAAAJ&hl=zh-CN">Jingping Wang</a>,
                  <a href="https://wangjingbo1219.github.io/">Jingbo Wang</a>,
                  <a href="http://dahua.site/">Dahua Lin</a>,
                  <a href="https://oceanpang.github.io/">Jiangmiao Pang</a>
                  <br>
                  (*=equal contribution)
                  <!-- <strong><em>CVPR</em>, 2025</strong> &nbsp -->
                  <!-- <font color="red"><strong>(Oral Presentation)</strong> -->
                  <!-- </font> -->
                  <br>
                  <a href="https://gallantloco.github.io">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2511.14625">arxiv</a>
                  <!-- / -->
                  <!-- <a href="https://github.com/ManipTrans/ManipTrans">Github</a> -->
                  <p></p>
                  <p>
                    <span style="font-variant: small-caps;">Gallant</span> is a voxel-grid‚Äìbased perception-learning
                    framework that uses LiDAR voxelization and a
                    z-grouped 2D CNN to learn end-to-end humanoid locomotion policies in complex 3D constrained
                    environments. By providing global 3D coverage beyond elevation maps and depth images, and training
                    in a high-fidelity LiDAR simulator, a single <span style="font-variant: small-caps;">Gallant</span>
                    policy robustly handles stairs, narrow
                    passages, lateral clutter, overhead constraints, and multi-level structures with near-perfect
                    success.
                  </p>
                </td>
              </tr>

              <tr onmouseout="maniptrans_stop()" onmouseover="maniptrans_start()">
                <td style="padding:10%;width:35%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='maniptrans_video'><video width="130%" muted autoplay loop>
                        <source src="images/maniptrans_cut.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/maniptrans_teaser.png' width="130%">
                  </div>
                  <script type="text/javascript">
                    function maniptrans_start() {
                      document.getElementById('maniptrans_video').style.opacity = "1";
                    }

                    function maniptrans_stop() {
                      document.getElementById('maniptrans_video').style.opacity = "0";
                    }
                    maniptrans_stop()
                  </script>
                </td>
                <td style="padding:0px;width:55%;vertical-align:middle">
                  <a href="#">
                    <papertitle><span style="font-variant: small-caps;">ManipTrans</span>: Efficient Dexterous Bimanual
                      Manipulation Transfer via Residual Learning
                    </papertitle>
                  </a>
                  <br>
                  <strong>Kailin Li</strong>,
                  <a href="https://xiaoyao-li.github.io/">Puhao Li</a>,
                  <a href="https://tengyu.ai/">Tengyu Liu</a>,
                  <a href="https://yuyangli.com/">Yuyang Li</a>,
                  <a href="https://siyuanhuang.com/">Siyuan Huang</a>
                  <br>
                  <strong><em>CVPR</em>, 2025</strong> &nbsp
                  <!-- <font color="red"><strong>(Oral Presentation)</strong> -->
                  <!-- </font> -->
                  <br>
                  <a href="https://maniptrans.github.io/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2503.21860">arxiv</a>
                  /
                  <a href="https://github.com/ManipTrans/ManipTrans">Github</a>
                  /
                  <a href="https://mp.weixin.qq.com/s/U1mqo5TTQ19qc_C8VzBC9A">Overview(‰∏≠Êñá)</a>
                  /
                  <a href="https://event.baai.ac.cn/activities/930">Talk(‰∏≠Êñá)</a>
                  <p></p>
                  <p>
                    Our objective is to develop a policy that enables dexterous robotic hands to replicate human hands
                    accurately‚Äìobject interaction trajectories in simulation while satisfying the tasks' semantic
                    manipulation constraints. The key innovation of <span
                      style="font-variant: small-caps;">ManipTrans</span> is to frame this transfer as a
                    two-stage
                    process: first, a pre-training trajectory imitation stage focusing solely on hand motion, and
                    second, a specific action fine-tuning stage that addresses interaction constraints. By leveraging
                    <span style="font-variant: small-caps;">ManipTrans</span>, we transfer multiple hand‚Äìobject
                    datasets to robotic hands, creating <span style="font-variant: small-caps;">DexManipNet</span>‚Äîa
                    large-scale dataset featuring previously unexplored tasks such as pen capping and bottle unscrewing,
                    that facilitate further policy training for dexterous hands and enabling real-world deployments.
                  </p>
                </td>
              </tr>

              <tr onmouseout="semgrasp_stop()" onmouseover="semgrasp_start()">
                <td style="padding:10%;width:35%;vertical-align:middle">
                  <div class="one">
                    <!-- <div class="two" id='semgrasp_video'><video width="100%" muted autoplay loop>
                        <source src="images/FAVOR_dataset_cut.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div> -->
                    <img src='images/semgrasp_teaser.png' width="110%">
                  </div>
                  <!-- <script type="text/javascript">
                    function favor_start() {
                      document.getElementById('favor_video').style.opacity = "1";
                    }

                    function favor_stop() {
                      document.getElementById('favor_video').style.opacity = "0";
                    }
                    favor_stop()
                  </script> -->
                </td>
                <td style="padding:0px;width:55%;vertical-align:middle">
                  <a href="#">
                    <papertitle><span style="font-variant: small-caps;">SemGrasp</span>: Semantic Grasp Generation via
                      Language Aligned Discretization
                    </papertitle>
                  </a>
                  <br>
                  <strong>Kailin Li</strong>,
                  <a href="https://scholar.google.com/citations?hl=en&user=GStTsxAAAAAJ">Jingbo Wang</a>,
                  <a href="https://lixiny.github.io">Lixin Yang</a>,
                  <a href="https://www.mvig.org">Cewu Lu</a>,
                  <a href="http://daibo.info">Bo Dai</a>
                  <br>
                  <strong><em>ECCV</em>, 2024</strong> &nbsp <font color="red"><strong>(Oral Presentation)</strong>
                  </font>
                  <br>
                  <a href="https://kailinli.github.io/SemGrasp/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2404.03590">arxiv</a>
                  /
                  <a href="https://huggingface.co/datasets/LiKailin/CapGrasp">dataset</a>
                  <p></p>
                  <p>
                    Generating human grasps involves both object geometry and semantic cues. This paper introduces
                    <span style="font-variant: small-caps;">SemGrasp</span>, a method that infuses semantic information
                    into grasp generation, aligning with language
                    instructions. Leveraging a unified semantic framework and a Multimodal Large Language Model (MLLM),
                    <span style="font-variant: small-caps;">SemGrasp</span> is supported by <span
                      style="font-variant: small-caps;">CapGrasp</span>, a dataset
                    featuring detailed captions and diverse grasps.
                    Experiments demonstrate <span style="font-variant: small-caps;">SemGrasp</span>'s ability to produce
                    grasps consistent with linguistic intentions,
                    surpassing shape-only approaches.
                  </p>
                </td>
              </tr>

              <tr onmouseout="favor_stop()" onmouseover="favor_start()">
                <td style="padding:10%;width:35%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='favor_video'><video width="120%" muted autoplay loop>
                        <source src="images/FAVOR_dataset_cut.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/favor_start.png' width="120%">
                  </div>
                  <script type="text/javascript">
                    function favor_start() {
                      document.getElementById('favor_video').style.opacity = "1";
                    }

                    function favor_stop() {
                      document.getElementById('favor_video').style.opacity = "0";
                    }
                    favor_stop()
                  </script>
                </td>
                <td style="padding:0px;width:55%;vertical-align:middle">
                  <a href="#">
                    <papertitle><span style="font-variant: small-caps;">Favor</span>: Full-Body AR-Driven Virtual Object
                      Rearrangement Guided by Instruction Text
                    </papertitle>
                  </a>
                  <br>
                  <strong>Kailin Li</strong>,
                  <a href="">Lixin Yang</a>,
                  <a href="">Zenan Lin</a>,
                  <a href="">Jian Xu</a>,
                  <a href="">Xinyu Zhan</a>,
                  <a href="">Yifei Zhao</a>,
                  <a href="">Pengxiang Zhu</a>,
                  <a href="">Wenxiong Kang</a>,
                  <a href="">Kejian Wu</a>,
                  <a href="https://www.mvig.org">Cewu Lu</a>
                  <br>
                  <strong><em>AAAI</em>, 2024</strong>
                  <br>
                  <a href="https://kailinli.github.io/FAVOR/">project page</a>
                  <!-- /
                  <a href="">arxiv</a> -->
                  <p></p>
                  <p>
                    Rearranging objects is key in human-environment interaction, and creating natural sequences of such
                    motions is crucial in AR/VR and CG. Our work presents <span
                      style="font-variant: small-caps;">Favor</span>, a unique dataset that captures
                    full-body virtual object rearrangement motions through motion capture and AR glasses. We also
                    introduce a new pipeline, <span style="font-variant: small-caps;">Favorite</span>, for generating
                    lifelike digital human rearrangement motions
                    driven by commands. Our experiments show that <span style="font-variant: small-caps;">Favor</span>
                    and <span style="font-variant: small-caps;">Favorite</span> produce high-fidelity motion
                    sequences.
                  </p>
                </td>
              </tr>

              <tr onmouseout="chord_stop()" onmouseover="chord_start()">
                <td style="padding:10%;width:35%;vertical-align:middle">
                  <div class="one">
                    <!-- <div class="two" id='chord_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/chord_teaser.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div> -->
                    <!-- <img src='images/chord_teaser.mp4' width="100%">
                  </div> -->
                    <div class="two" id='chord_video'><video width="110%" muted autoplay loop>
                        <source src="images/chord_teaser.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video>
                    </div>
                    <img src='images/chord_before.png' width="110%">
                    <script type="text/javascript">
                      function chord_start() {
                        document.getElementById('chord_video').style.opacity = "1";
                      }

                      function chord_stop() {
                        document.getElementById('chord_video').style.opacity = "0";
                      }
                      chord_stop()
                    </script>
                </td>
                <td style="padding:0px;width:55%;vertical-align:middle">
                  <a href="#">
                    <papertitle><span style="font-variant: small-caps;">Chord</span>: Category-level in-Hand Object
                      Reconstruction via Shape Deformation
                    </papertitle>
                  </a>
                  <br>
                  <strong>Kailin Li</strong>,
                  <a href="">Lixin Yang</a>,
                  <a href="">Haoyu Zhen</a>
                  <a href="">Zenan Lin</a>,
                  <a href="">Xinyu Zhan</a>,
                  <a href="">Licheng Zhong</a>,
                  <a href="">Jian Xu</a>,
                  <a href="">Kejian Wu</a>,
                  <a href="https://www.mvig.org">Cewu Lu</a>
                  <br>
                  <strong><em>ICCV</em>, 2023</strong>
                  <br>
                  <a href="https://kailinli.github.io/CHORD/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2308.10574">arxiv</a>
                  /
                  <a href="https://huggingface.co/datasets/LiKailin/COMIC">dataset</a>
                  /
                  <a href=https://github.com/anyeZHY/PyBlend">PyBlend</a>
                  <p></p>
                  <p>
                    We proposed a new method <span style="font-variant: small-caps;">Chord</span> which exploits the
                    categorical shape prior for reconstructing the
                    shape of intra-class objects. In addition, we constructed a new dataset, COMIC, of category-level
                    hand-object interaction. <span style="font-variant: small-caps;">Comic</span> encompasses a diverse
                    collection of object instances, materials, hand
                    interactions, and viewing directions, as illustrated.
                  </p>
                </td>
              </tr>

              <tr onmouseout="oakink_stop()" onmouseover="oakink_start()">
                <td style="padding:10%;width:35%;vertical-align:middle;text-align: center;">
                  <div class="one">
                    <div class="two" id='oakink_image'>
                      <img src='images/oakink_after.png' width="110%">
                    </div>
                    <img src='images/oakink_before.png' width="110%">
                  </div>
                  <script type="text/javascript">
                    function oakink_start() {
                      document.getElementById('oakink_image').style.opacity = "1";
                    }

                    function oakink_stop() {
                      document.getElementById('oakink_image').style.opacity = "0";
                    }
                    oakink_stop()
                  </script>
                </td>
                <td style="padding:0px;width:55%;vertical-align:middle">
                  <a href="#">
                    <papertitle>OakInk: A Large-scale Knowledge Repository for Understanding Hand-Object Interaction
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://lixiny.github.io">Lixin Yang</a>*,
                  <strong>Kailin Li</strong>*,
                  <a href="#">Xinyu Zhan</a>*,
                  <a href="#">Fei Wu</a>,
                  <a href="https://anran-xu.github.io">Anran Xu</a>,
                  <a href="https://liuliu66.github.io">Liu Liu</a>,
                  <a href="https://www.mvig.org">Cewu Lu</a>
                  <br>
                  (*=equal contribution)
                  <br>
                  <strong><em>CVPR</em>, 2022</strong>
                  <br>
                  <a href="https://github.com/lixiny/OakInk">project</a>
                  /
                  <a href="https://arxiv.org/abs/2203.15709">arxiv</a>
                  /
                  <a href="https://github.com/lixiny/OakInk">code</a>
                  /
                  <a href="https://github.com/lixiny/OakInk">dataset</a>
                  <p></p>
                  <p>
                    Learning how humans manipulate objects requires machines to acquire knowledge from two perspectives:
                    one for understanding object affordances and the other for learning human interactions based on
                    affordances. In this work, we propose a multi-modal and rich-annotated knowledge repository,
                    <strong>OakInk</strong>,
                    for the visual and cognitive understanding of hand-object interactions. Check our website for more
                    details!
                  </p>
                </td>
              </tr>

              <tr onmouseout="artiboost_stop()" onmouseover="artiboost_start()">
                <td style="padding:10%;width:35%;vertical-align:middle;text-align: center;">
                  <div class="one">
                    <div class="two" id='artiboost_image'>
                      <img src='images/artiboost_after.jpg' width="110%">
                    </div>
                    <img src='images/artiboost_before.png' width="110%">
                  </div>
                  <script type="text/javascript">
                    function artiboost_start() {
                      document.getElementById('artiboost_image').style.opacity = "1";
                    }

                    function artiboost_stop() {
                      document.getElementById('artiboost_image').style.opacity = "0";
                    }
                    artiboost_stop()
                  </script>
                </td>
                <td style="padding:0px;width:55%;vertical-align:middle">
                  <a href="#">
                    <papertitle>ArtiBoost: Boosting Articulated 3D Hand-Object Pose Estimation via Online Exploration
                      and Synthesis
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://lixiny.github.io">Lixin Yang</a>*,
                  <strong>Kailin Li</strong>*,
                  <a href="#">Xinyu Zhan</a>,
                  <a href="https://lyuj1998.github.io">Jun Lv</a>,
                  <a href="https://wenqiangx.github.io/robotflowproject/">Wenqiang Xu</a>,
                  <a href="https://jeffli.site">Jiefeng Li</a>,
                  <a href="https://www.mvig.org">Cewu Lu</a>
                  <br>
                  (*=equal contribution)
                  <br>
                  <strong><em>CVPR</em>, 2022</strong> &nbsp <font color="red"><strong>(Oral Presentation)</strong>
                  </font>
                  <br>
                  <a href="#">project</a>
                  /
                  <a href="https://arxiv.org/abs/2109.05488">arxiv</a>
                  /
                  <a href="https://github.com/MVIG-SJTU/ArtiBoost">code</a>
                  <p></p>
                  <p>
                    We propose a lightweight online data enrichment method that boosts articulated hand-object pose
                    estimation
                    from the data perspective.
                    During training, ArtiBoost alternatively performs data exploration and synthesis.
                    Even with a simple baseline, our method can boost it to outperform the previous SOTA on several
                    hand-object benchmarks.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:10%;width:35%;vertical-align:middle;text-align: center;">
                  <div class="one">
                    <img src='images/DART.png' width="110%">
                  </div>
                </td>
                <td style="padding:0px;width:55%;vertical-align:middle">
                  <a href="#">
                    <papertitle>DART: Articulated Hand Model with Diverse Accessories and Rich Textures
                    </papertitle>
                  </a>
                  <br>
                  <a>Daiheng Gao</a>*,
                  <a href="https://xiuyuliang.cn">Yuliang Xiu</a>*,
                  <strong>Kailin Li</strong>*,
                  <a href="https://lixiny.github.io">Lixin Yang</a>*,
                  <br>
                  <a>Feng Wang</a>, <a>Peng Zhang</a>, <a>Bang Zhang</a>,
                  <a href="https://www.mvig.org">Cewu Lu</a>,
                  <a href="https://www.cs.sfu.ca/~pingtan/">Ping Tan</a>
                  <br>
                  (*=equal contribution)
                  <br>
                  <strong><em>NeurIPS</em>, 2022</strong> <em> - Datasets and Benchmarks Track</em>
                  <br>
                  <a href="https://dart20220.github.io">project (dataset)</a>
                  /
                  <a href="https://openreview.net/forum?id=FPgCB_Z_0O">paper</a>
                  /
                  <a href="#">arxiv</a>
                  /
                  <a href="https://github.com/DART2022/DARTset">code</a>
                  /
                  <a href=https://www.youtube.com/watch?v=VvlUYe-9b7U&feature=youtu.be">video</a>
                  <p></p>
                  <p>
                    In this paper, we extend MANO with more Diverse Accessories and Rich Textures, namely DART.
                    DART is comprised of 325 exquisite hand-crafted texture maps which vary in appearance and cover
                    different kinds of blemishes, make-ups, and accessories.
                    We also generate large-scale (800K), diverse, and high-fidelity hand images, paired with
                    perfect-aligned 3D labels, called DARTset.
                  </p>
                </td>
              </tr>

              <tr onmouseout="oakink2_stop()" onmouseover="oakink2_start()">
                <td style="padding:10%;width:35%;vertical-align:middle">
                  <div class="one">
                    <!-- <div class="two" id='favor_video'><video width="100%" muted autoplay loop>
                        <source src="images/FAVOR_dataset_cut.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div> -->
                    <div class="two" id="oakink2_img">
                      <img src='images/oakink2_after.png' width="110%">
                    </div>
                    <img src='images/oakink2_begin.png' width="110%">
                  </div>
                  <script type="text/javascript">
                    function oakink2_start() {
                      document.getElementById('oakink2_img').style.opacity = "1";
                    }

                    function oakink2_stop() {
                      document.getElementById('oakink2_img').style.opacity = "0";
                    }
                    oakink2_stop()
                  </script>
                </td>
                <td style="padding:0px;width:55%;vertical-align:middle">
                  <a href="#">
                    <papertitle>OAKINK2: A Dataset of Bimanual Hands-Object Manipulation in Complex Task Completion
                    </papertitle>
                  </a>
                  <br>
                  <a href="">Xinyu Zhan</a>*,
                  <a href="">Lixin Yang</a>*,
                  <a href="">Yifei Zhao</a>,
                  <a href="">Kangrui Mao</a>,
                  <a href="">Hanlin Xu</a>,
                  <a href="">Zenan Lin</a>,
                  <strong>Kailin Li</strong>,
                  <a href="https://www.mvig.org">Cewu Lu</a>
                  <br>
                  <strong><em>CVPR</em>, 2024</strong>
                  <br>
                  <a href="https://oakink.net/v2/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2403.19417">arxiv</a>
                  <p></p>
                  <p>
                    OAKINK2 is a rich dataset focusing on bimanual object manipulation tasks involved in complex daily
                    activities. It introduces a unique three-tiered abstraction structure‚ÄîAffordance, Primitive Task,
                    and Complex Task‚Äîto systematically organize task representations. By emphasizing an object-centric
                    approach, the dataset captures multi-view imagery and precise annotations of human and object poses,
                    aiding in applications like interaction reconstruction and motion synthesis. Furthermore, we propose
                    a Complex Task Completion framework that utilizes Large Language Models to break down complex
                    activities into Primitive Tasks and a Motion Fulfillment Model to generate corresponding bimanual
                    motions.
                  </p>
                </td>
              </tr>


              <tr onmouseout="refnerf_stop()" onmouseover="refnerf_start()">
                <td style="padding:10%;width:35%;vertical-align:middle;text-align: center;">
                  <div class="one">
                    <div class="two" id='colorneus_video'><video width="110%" muted autoplay loop>
                        <source src="images/Color_NeuS_ghostbear.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/Color_NeuS_ghostbear.jpg' width="110%">
                  </div>
                  </div>
                  <script type="text/javascript">
                    function refnerf_start() {
                      document.getElementById('colorneus_video').style.opacity = "1";
                    }

                    function refnerf_stop() {
                      document.getElementById('colorneus_video').style.opacity = "0";
                    }
                    refnerf_stop()
                  </script>
                </td>
                <td style="padding:0px;width:55%;vertical-align:middle">
                  <a href="#">
                    <papertitle>Color-NeuS: Reconstructing Neural Implicit Surfaces with Color
                    </papertitle>
                  </a>
                  <br>
                  <a>Licheng Zhong</a>*,
                  <a href="">Lixin Yang</a>*,
                  <strong>Kailin Li</strong>,
                  <a href="">Haoyu Zhen</a>
                  <a href="">Mei Han</a>,
                  <a href="https://www.mvig.org">Cewu Lu</a>,
                  <br>
                  <strong><em>3DV</em>, 2024</strong>
                  <br>
                  <a href="https://colmar-zlicheng.github.io/color_neus/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2308.06962">arxiv</a>
                  /
                  <a href="https://github.com/Colmar-zlicheng/Color-NeuS">code</a>
                  /
                  <a href=https://drive.google.com/drive/folders/1C6vcRiy_MFMtsaR8AydRAPrLIkok-tJm">data</a>
                  <p></p>
                  <p>
                    Color-NeuS focuses on mesh reconstruction with color. We remove view-dependent color while using a
                    relighting network to maintain volume rendering performance. Mesh is extracted from the SDF network,
                    and vertex color is derived from the global color network. We conceived a in hand object scanning
                    task and gathered several videos for it to evaluate Color-NeuS.
                  </p>
                </td>
              </tr>

              <tr onmouseout="cpf_stop()" onmouseover="cpf_start()">
                <td style="padding:10%;width:35%;vertical-align:middle;text-align: center;">
                  <div class="one">
                    <div class="two" id='cpf_image'>
                      <img src='images/cpf_after.png' width="110%">
                    </div>
                    <img src='images/cpf_before.jpg' width="110%">
                  </div>
                  <script type="text/javascript">
                    function cpf_start() {
                      document.getElementById('cpf_image').style.opacity = "1";
                    }

                    function cpf_stop() {
                      document.getElementById('cpf_image').style.opacity = "0";
                    }
                    cpf_stop()
                  </script>
                </td>
                <td style="padding:0px;width:55%;vertical-align:middle">
                  <a href="https://lixiny.github.io/CPF">
                    <papertitle>CPF: Learning a Contact Potential Field to Model the Hand-Object Interaction
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://lixiny.github.io">Lixin Yang</a>,
                  <a href="#">Xinyu Zhan</a>,
                  <strong>Kailin Li</strong>,
                  <a href="https://wenqiangx.github.io/robotflowproject/">Wenqiang Xu</a>,
                  <a href="https://jeffli.site">Jiefeng Li</a>,
                  <a href="https://www.mvig.org">Cewu Lu</a>
                  <br>
                  <strong><em>ICCV</em>, 2021</strong>
                  <br>
                  <a href="https://lixiny.github.io/CPF">project</a>
                  /
                  <a
                    href="https://openaccess.thecvf.com/content/ICCV2021/html/Yang_CPF_Learning_a_Contact_Potential_Field_To_Model_the_Hand-Object_ICCV_2021_paper.html">paper</a>
                  /
                  <a
                    href="https://openaccess.thecvf.com/content/ICCV2021/supplemental/Yang_CPF_Learning_a_ICCV_2021_supplemental.pdf">supp</a>
                  /
                  <a href="https://arxiv.org/abs/2012.00924">arxiv</a>
                  /
                  <a href="https://github.com/lixiny/CPF">code</a>
                  /
                  <a href="https://zhuanlan.zhihu.com/p/406470702">Áü•‰πé</a>
                  <p></p>
                  <p>
                    We highlight contact in the hand-object interaction modeling task by proposing an
                    explicit representation named Contact Potential Field (CPF). In CPF, we treat each contacting
                    hand-object
                    vertex pair as a spring-mass system, Hence the whole system forms a potential field with minimal
                    elastic
                    energy
                    at the grasp position.</p>
                </td>
              </tr>

              <!-- <tr onmouseout="refnerf_stop()" onmouseover="refnerf_start()">
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='refnerf_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/refnerf.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/refnerf.jpg' width="160">
                  </div>
                  <script type="text/javascript">
                    function refnerf_start() {
                      document.getElementById('refnerf_image').style.opacity = "1";
                    }

                    function refnerf_stop() {
                      document.getElementById('refnerf_image').style.opacity = "0";
                    }
                    refnerf_stop()
                  </script>
                </td>
                <td style="padding:20px;width:55%;vertical-align:middle">
                  <a href="https://dorverbin.github.io/refnerf/index.html">
                    <papertitle>Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields</papertitle>
                  </a>
                  <br>
                  <a href="https://scholar.harvard.edu/dorverbin/home">Dor Verbin</a>,
                  <a href="https://phogzone.com/">Peter Hedman</a>,
                  <a href="https://bmild.github.io/">Ben Mildenhall</a>, <br>
                  <a href="Todd Zickler">Todd Zickler</a>,
                  <strong>Jonathan T. Barron</strong>,
                  <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>
                  <br>
                  <em>CVPR</em>, 2022
                  <br>
                  <a href="https://dorverbin.github.io/refnerf/index.html">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2112.03907">arXiv</a>
                  /
                  <a href="https://youtu.be/qrdRH9irAlk">video</a>
                  <p></p>
                  <p>Explicitly modeling reflections in NeRF produces realistic shiny surfaces and accurate surface
                    normals, and lets you edit materials.</p>
                </td>
              </tr>

              <tr>

              <tr onmouseout="mipnerf_stop()" onmouseover="mipnerf_start()" bgcolor="#ffffd0">
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='mipnerf_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/mipnerf_ipe_yellow.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/mipnerf_ipe_yellow.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function mipnerf_start() {
                      document.getElementById('mipnerf_image').style.opacity = "1";
                    }

                    function mipnerf_stop() {
                      document.getElementById('mipnerf_image').style.opacity = "0";
                    }
                    mipnerf_stop()
                  </script>
                </td>
                <td style="padding:20px;width:55%;vertical-align:middle">
                  <a href="http://jonbarron.info/mipnerf">
                    <papertitle>Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields
                    </papertitle>
                  </a>
                  <br>
                  <strong>Jonathan T. Barron</strong>,
                  <a href="https://bmild.github.io/">Ben Mildenhall</a>,
                  <a href="http://matthewtancik.com/">Matthew Tancik</a>, <br>
                  <a href="https://phogzone.com/">Peter Hedman</a>,
                  <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>,
                  <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>
                  <br>
                  <em>ICCV</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Honorable
                      Mention)</strong></font>
                  <br>
                  <a href="http://jonbarron.info/mipnerf">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2103.13415">arXiv</a>
                  /
                  <a href="https://youtu.be/EpH175PY1A0">video</a>
                  /
                  <a href="https://github.com/google/mipnerf">code</a>
                  <p></p>
                  <p>NeRF is aliased, but we can anti-alias it by casting cones and prefiltering the positional encoding
                    function.</p>
                </td>
              </tr>

              <tr onmouseout="survey_stop()" onmouseover="survey_start()">
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='survey_image'>
                      <img src='images/survey_after.png' width="160">
                    </div>
                    <img src='images/survey_before.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function survey_start() {
                      document.getElementById('survey_image').style.opacity = "1";
                    }

                    function survey_stop() {
                      document.getElementById('survey_image').style.opacity = "0";
                    }
                    survey_stop()
                  </script>
                </td>
                <td style="padding:20px;width:55%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2111.05849">
                    <papertitle>Advances in Neural Rendering</papertitle>
                  </a>
                  <br>
                  <a href="https://people.mpi-inf.mpg.de/~atewari/">Ayush Tewari</a>,
                  <a href="https://justusthies.github.io/">Justus Thies</a>,
                  <a href="https://bmild.github.io/">Ben Mildenhall</a>,
                  <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
                  <a href="https://people.mpi-inf.mpg.de/~tretschk/">Edgar Tretschk</a>,
                  <a href="https://homes.cs.washington.edu/~yifan1/">Yifan Wang</a>,
                  <a href="https://christophlassner.de/">Christoph Lassner</a>,
                  <a href="https://vsitzmann.github.io/">Vincent Sitzmann</a>,
                  <a href="http://ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>,
                  <a href="https://stephenlombardi.github.io/">Stephen Lombardi</a>,
                  <a href="http://www.cs.cmu.edu/~tsimon/">Tomas Simon</a>,
                  <a href="https://www.mpi-inf.mpg.de/departments/visual-computing-and-artificial-intelligence">Christian
                    Theobalt</a>,
                  <a href="https://www.niessnerlab.org/">Matthias Niessner</a>,
                  <strong>Jonathan T. Barron</strong>,
                  <a href="https://stanford.edu/~gordonwz/">Gordon Wetzstein</a>,
                  <a href="https://zollhoefer.com/">Michael Zollhoefer</a>,
                  <a href="https://people.mpi-inf.mpg.de/~golyanik/">Vladislav Golyanik</a>
                  <br>
                  <em>Arxiv</em>, 2021
                  <br>
                  <p></p>
                  <p>
                    A survey of recent progress in neural rendering.
                  </p>
                </td>
              </tr>

              <tr onmouseout="jump_stop()" onmouseover="jump_start()" bgcolor="#ffffd0">
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='jump_image'><img src='images/jump_anim.gif'></div>
                    <img src='images/jump_still.png'>
                  </div>
                  <script type="text/javascript">
                    function jump_start() {
                      document.getElementById('jump_image').style.opacity = "1";
                    }

                    function jump_stop() {
                      document.getElementById('jump_image').style.opacity = "0";
                    }
                    jump_stop()
                  </script>
                </td>
                <td style="padding:20px;width:55%;vertical-align:middle">
                  <a href="https://drive.google.com/file/d/1RBnTrtzqmuO8uj3GQaR5vBJZjIC3Jxjn/view?usp=sharing">
                    <papertitle>Jump: Virtual Reality Video</papertitle>
                  </a>
                  <br>
                  <a href="http://mi.eng.cam.ac.uk/~ra312/">Robert Anderson</a>, <a
                    href="https://www.cs.unc.edu/~gallup/">David Gallup</a>, <strong>Jonathan T. Barron</strong>, <a
                    href="https://mediatech.aalto.fi/~janne/index.php">Janne Kontkanen</a>, <a
                    href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>, <a
                    href="http://carlos-hernandez.org/">Carlos Hern&aacutendez</a>, <a
                    href="https://homes.cs.washington.edu/~sagarwal/">Sameer Agarwal</a>, <a
                    href="https://homes.cs.washington.edu/~seitz/">Steven M Seitz</a>
                  <br>
                  <em>SIGGRAPH Asia</em>, 2016
                  <br>
                  <a
                    href="https://drive.google.com/file/d/11D4eCDXqqFTtZT0WS2COJE0hsAN3QEww/view?usp=sharing">supplement</a>
                  /
                  <a href="https://www.youtube.com/watch?v=O0qUYynupTI">video</a> /
                  <a href="data/Anderson2016.bib">bibtex</a> /
                  <a href="https://blog.google/products/google-vr/jump-using-omnidirectional-stereo-vr-video/">blog
                    post</a>
                  <p></p>
                  <p>Using computer vision and a ring of cameras, we can make video for virtual reality headsets that is
                    both stereo and 360&deg;.</p>
                  <p>This technology is used by <a href="https://vr.google.com/jump/">Jump</a>. </p>
                </td>
              </tr> -->

            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>Misc</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>
              <tr>
                <!-- <td style="padding:20px;width:35%;vertical-align:middle"><img src="images/cvf.jpg"></td> -->
                <td width="75%" valign="center">
                  International top-tier conference workshop <a
                    href="https://hands-workshop.org/workshop2025.html"><strong>HANDS Workshop @ ICCV 2025</strong></a>
                  <strong>Co-organizer</strong>: Lead, Embodied Manipulation Challenge.
                </td>
                <td width="75%" valign="center">
                  Conference reviewer for
                  <strong>ICLR25/26</strong>,<strong>ICML25</strong>,<strong>NeurIPS24/25</strong>,<strong>CVPR22/24/25</strong>,<strong>ICCV23</strong>,<strong>ECCV22/24</strong>,
                  <strong>AAAI25</strong>, <strong>3DV22</strong>.
                  <!-- <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
                  <br>
                  <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Longuet-Higgins Award Committee Member,
                    CVPR 2021</a>
                  <br>
                  <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
                  <br>
                  <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a> -->
                </td>
              </tr>
              <!-- <tr>
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src="images/cs188.jpg" alt="cs188">
                </td>
                <td width="75%" valign="center">
                  <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor,
                    CS188 Spring 2011</a>
                  <br>
                  <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor,
                    CS188 Fall 2010</a>
                  <br>
                  <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd
                    Edition</a>
                </td>
              </tr>

              <tr>
                <td align="center" style="padding:20px;width:35%;vertical-align:middle">
                  <heading>Basically <br> Blog Posts</heading>
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                  <br>
                  <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain
                    Functions</a>
                  <br>
                  <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                </td>
              </tr> -->

            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>